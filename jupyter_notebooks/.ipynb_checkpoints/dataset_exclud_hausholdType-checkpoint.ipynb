{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a15628b-c5e0-4be5-b4ed-a9910091ab76",
   "metadata": {},
   "source": [
    "# Dataset Transform - excluding Household Type\n",
    "\n",
    "In this notebook we are going to transform the data and vcreate a new (csv or parquet?) file that contains the total emissions per type of mobility  (`emissions_X`) and the development of choices (`stock_X`), where `X` can be any of the following types of mobility:\n",
    "- `C`: combustion car\n",
    "- `E`: electric car\n",
    "- `N`: non-motorized.\n",
    "- `P`: public transport.\n",
    "- `S`: shared mobility.\n",
    "The idea is to retrive tables like the following (I am actually thinking of making two csv files, one with the emissions and the other with the stock development):\n",
    "\n",
    "| Step | reID | VarName | Value (total of regions) |\n",
    "| --- | --- | --- | --- |\n",
    "| 0 | 2335 | `emissions_C` | $\\sum_{i=1}^{11}Chh_i$ |\n",
    "| 0 | 2335 | `emissions_E` | $\\sum_{i=1}^{11}Ehh_i$ |\n",
    "| 0 | 2335 | `emissions_N` | $\\sum_{i=1}^{11}Nhh_i$  |\n",
    "| 0 | 2335 | `emissions_P` | $\\sum_{i=1}^{11}Phh_i$  |\n",
    "| 0 | 2335 | `emissions_S` | $\\sum_{i=1}^{11}Shh_i$  |\n",
    "\n",
    "There are a total of 181 steps (from 0 to 180), and for each mobility choice (there are 5 mobility choices), there are two variables regarding emissions and stock development, and there are 16 regions. Thus, the total number of rows is\n",
    "$$\\text{number of rows: }16\\times 181\\times 2\\times 5=28960$$\n",
    "Right now, each .csv files has **324352** rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a4369f-ac11-44a7-8821-88183dafe6e7",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "These are the ideas to make this iteratively:\n",
    "\n",
    "1. Find a way in which I can get a list of the names of the files so that later on, we can iterate through them.\n",
    "2. For each file, create a dataframe, and then\n",
    "3. Transform the dataframe so that we ignore (for now) the types of household.\n",
    "\n",
    "I even propose to start wsith a random file, do point 2, and once that is ready, we proceed to do it for all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b590a4-682f-49b3-bca1-796e5ae9ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632a5299-fcbc-47f0-94a9-6c86a0118cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/moni/Documents/motmo/timeSeries_files/' # original data\n",
    "PATH2 = '/home/moni/Documents/motmo/data_without_hhID/' # folder in which we will store transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b245f864-7f81-4021-8ddb-10d3c55458bc",
   "metadata": {},
   "source": [
    "### For the record\n",
    "\n",
    "The file we chose at random is the choice in which there is \"Investment in public transport\" (`SP`) and in \"Electric vehicel subsidy\" (`SE`), and there is the *event* in which the gas price is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6caf104-c5c2-41ef-84cd-7dd8e0e8a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_df = pd.read_csv(PATH + 'timeSeries_CH0SP1SE1WE0BP0RE0CO1DI0WO0CS0.csv')\n",
    "hh_df = hh_df[hh_df.varName != 'elDem']\n",
    "hh_df = hh_df[hh_df.varName != 'nStat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81c8a8-81db-4957-9a8a-d424f74471ab",
   "metadata": {},
   "source": [
    "### Ignoring Household Type\n",
    "We will ignore the household type and will sum all over the timesteps, regardless of `hhID`. For that, we create a function called `del_hhID_from_df(hh_df)` that takes as input the original dataframe and returns the desired one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49fdeb0-d376-4967-9097-a4686780a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_hhID_from_df(hh_df):\n",
    "    hh_df = hh_df[hh_df.varName != 'elDem']\n",
    "    hh_df = hh_df[hh_df.varName != 'nStat']\n",
    "    hh_df = hh_df.groupby(['step','reID','varName']).sum().reset_index()\n",
    "    del hh_df['hhID']\n",
    "    hh_df.head(12)\n",
    "    return hh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9a9892-6743-4c7d-a263-ec452882762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_df_to_csv(df,file_name):\n",
    "    df.to_csv(PATH2 + file_name)# saves in the new location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b580c7-76a1-49af-b981-0a6429d0226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_df_to_csv(hh_df,\"prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e48b66-b268-49b6-8735-2e8844f34d7c",
   "metadata": {},
   "source": [
    "## Retreiving all file names\n",
    "We want a list with all file names of each of the 539 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ccfa06-c476-4886-9d4e-69cb848e2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_file_names():\n",
    "    file_names = [f for f in listdir(PATH) if isfile(join(PATH, f))]\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49abe935-e6ea-4e7d-9b09-f0cad0c1ca29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_n = list_file_names().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994e0135-8f67-48cb-a67a-cda1ff0b30da",
   "metadata": {},
   "source": [
    "### Saving all files\n",
    "This function takes as input the converted dataframe.\n",
    "\n",
    "CAREFUL!!! only run this once, since it takes some minutes and if you do it and the files are already there, there might be overlapping or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c5fa44-3a09-497e-9aa0-c7838ae218f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def from_csv_to_df(file_name):\n",
    "#     df = pd.read_csv(PATH + file_name)\n",
    "#     return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0441c8-3832-42a5-92d7-e5748bb72e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_all_files(file_name_list):\n",
    "    n = len(file_name_list)\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        df = pd.read_csv(PATH + file_name_list[i])\n",
    "        df = del_hhID_from_df(df)\n",
    "        from_df_to_csv(df,file_name_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b54bd5-c656-42fa-b02c-7f640eaf6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_all_files(file_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2622f7a-fad7-4cda-aec6-621a831d13b9",
   "metadata": {},
   "source": [
    "### Let's try something\n",
    "I will try to save all these .csv files into one parquet file\n",
    "\n",
    "IDEA FOR NEXT TIME: from the list that contains all names, create a new list that deletes the \".csv\" extension at the end, and also the \"TimeSeries\" that each file name starts with, and then try to do what has been donde here to save it all in one parquet file: https://stackoverflow.com/questions/63509110/convert-multiple-csvs-to-single-partitioned-parquet-dataset\n",
    "\n",
    "The code below is not complete, do not run!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c070f7-9ca9-454d-ba2e-f293c9c7ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_n = rlist_file_names().copy() # we need a new list!!!\n",
    "for f in file_n:\n",
    "    df = pd.read_csv(f)\n",
    "    df = del_hhID_from_df(df).drop(columns=['value'])\n",
    "    df.to_parquet(f'all_years.pq/YEAR={year}')\n",
    "fastparquet.writer.merge([f'all_years.pq/YEAR={y}' for y in years])\n",
    "\n",
    "df_all = pd.read_parquet('all_years.pq')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
